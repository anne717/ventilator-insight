{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script for function to patient info data\n",
    "\n",
    "This function loads chart events (specified by user-entered ID) into a dataframe, filters events by ventilataed patients and places different chart events into different rows in df/\n",
    "\n",
    "#### import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print virtual memory available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svmem = psutil.virtual_memory()\n",
    "print (svmem.available) #in bytes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print size of database we're pulling from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.getsize('./chartevents.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### figure out chunk size for pandas dataframe reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv('./chartevents.csv', nrows=10)\n",
    "df_sample_size = df_sample.memory_usage(index=True).sum()\n",
    "my_chunk = (2000000000 / df_sample_size)/10\n",
    "my_chunk = int(my_chunk//1) # we get the integer part\n",
    "print (my_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) create dataframe structure and set chunksize for iterating data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_csv = pd.read_csv(\n",
    "    'chartevents.csv',\n",
    "    iterator=True,\n",
    "    chunksize=my_chunk,\n",
    "    dtype={'subject_id': int, 'hadm_id': int, 'stay_id': int, \n",
    "           'charttime' : str, 'storetime': str, 'itemid': int,\n",
    "           'value': str, 'valuenum': float, 'valueuom': str, 'warning': int})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) get chart events data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = pd.concat(\n",
    "    [chunk[(chunk['itemid'] == 220003)|(chunk['itemid'] == 226228)|(chunk['itemid'] == 226545)|\n",
    "           (chunk['itemid'] == 226515)|(chunk['itemid'] == 226724)|(chunk['itemid'] == 227088)|\n",
    "           (chunk['itemid'] == 224639)|(chunk['itemid'] == 226531)|(chunk['itemid'] == 226707)|\n",
    "           (chunk['itemid'] == 226730)]\n",
    "        for chunk in iter_csv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get ids of patients that were ventilated and seletc these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svents = pd.read_csv('sample_vents.csv')\n",
    "ids = svents['hadm_id']\n",
    "df_result = df_result[df_result['hadm_id'].isin(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get weight of patients (where duplicate values exist, take average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = df_result[df_result['itemid']==226531]\n",
    "avg_dupValues = weight.groupby('hadm_id')['valuenum'].mean()\n",
    "meanWeight = avg_dupValues.reset_index()\n",
    "meanWeight.columns = (['hadm_id','weight'])\n",
    "meanWeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get height of patients and convert height in cm to inches (where duplicate values exist, take average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = df_result[(df_result['itemid']==226730)|(df_result['itemid']==226707)]\n",
    "\n",
    "# convert height in cm to inches- divide by 2.74\n",
    "height['valuenum'] = np.where(height['itemid']==226730,(height['valuenum']/2.54),height['valuenum'])\n",
    "avg_dupValues = height.groupby('hadm_id')['valuenum'].mean()\n",
    "meanHeight = avg_dupValues.reset_index()\n",
    "meanHeight.columns = (['hadm_id','height'])\n",
    "meanHeight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load feature table and add height and weight to it\n",
    "\n",
    "make sure feature table only includes correct hadm_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_table = pd.read_csv('feature_table.csv')\n",
    "feature_table = feature_table[feature_table['hadm_id'].isin(ids)]\n",
    "new_df = pd.merge(left = feature_table, right = meanWeight, how = 'left', left_on='hadm_id', right_on='hadm_id')\n",
    "new_df =pd.merge(left = new_df, right = meanHeight, how = 'left', left_on='hadm_id', right_on='hadm_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the admissions data for patient that were ventilated, select features that might be useful in the model or later\n",
    "\n",
    "1) For 'value' (i.e. length of time patient was on ventilator), convert all readings into hours\n",
    "\n",
    "2) rename 'value' column as 'time)on_vent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svents_keep = svents[['hadm_id', 'endtime', 'value', 'valueuom', 're_intub_class',\n",
    "       'subject_id', 'admittime', 'deathtime', 'admission_type',\n",
    "       'admission_location','marital_status', 'ethnicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svents_keep['value'] = np.where(svents_keep['valueuom']=='min',(svents_keep['value']/60),svents_keep['value'])\n",
    "svents_keep['value'] = np.where(svents_keep['valueuom']=='day',(svents_keep['value']*24),svents_keep['value'])\n",
    "svents_keep.drop('valueuom',axis=1,inplace=True)\n",
    "svents_keep.rename({\"value\":\"time_on_vent\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add admission table data to feature table and save csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_df =pd.merge(left = new_df, right = svents_keep, how = 'left', left_on='hadm_id', right_on='hadm_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_df.to_csv('all_feature_table.csv',index= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
